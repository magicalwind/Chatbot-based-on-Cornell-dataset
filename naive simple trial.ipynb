{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the NN in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1 = torch.nn.Conv2d(...)\n",
    "torch.nn.init.xavier_uniform(conv1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for LSTM\n",
    "def init_hidden(self, batch_size):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For entire module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        xavier(m.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "v_embeddings.weight.data.uniform_(-0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operation of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = torch.arange(9)\n",
    "v = v.view(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.],\n",
      "        [ 4.,  3.],\n",
      "        [ 8.,  7.]])\n"
     ]
    }
   ],
   "source": [
    "# Gather element\n",
    "# torch.gather(input, dim, index, out=None)\n",
    "# out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n",
    "# out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n",
    "# out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n",
    "\n",
    "# 0  1\n",
    "# 4  3\n",
    "# 8  7\n",
    "r = torch.gather(v, 1, torch.LongTensor([[0,1],[1,0],[2,1]]))\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  2.],\n",
      "        [ 3.,  5.],\n",
      "        [ 6.,  8.]])\n"
     ]
    }
   ],
   "source": [
    "# Index select\n",
    "# 0 2\n",
    "# 3 5\n",
    "# 6 8\n",
    "indices = torch.LongTensor([0, 2])\n",
    "r = torch.index_select(v, 1, indices)\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.,  4.,  5.,  6.,  7.,  8.])\n"
     ]
    }
   ],
   "source": [
    "mask = v.ge(3)\n",
    "r = torch.masked_select(v, mask)\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1],\n",
      "        [ 0,  2],\n",
      "        [ 1,  0],\n",
      "        [ 1,  1],\n",
      "        [ 1,  2],\n",
      "        [ 2,  0],\n",
      "        [ 2,  1],\n",
      "        [ 2,  2]])\n"
     ]
    }
   ],
   "source": [
    "# Note to get nonzero pairs with numpy.non_zero, we need to add a transpose\n",
    "r = torch.nonzero(v)\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.5000,  0.5000],\n",
      "        [ 0.5000,  0.5000,  0.5000],\n",
      "        [ 0.5000,  0.5000,  0.5000]])\n"
     ]
    }
   ],
   "source": [
    "r = torch.clamp(v, min=-0.5, max=0.5)\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10.,  11.,  12.],\n",
      "        [ 13.,  14.,  15.],\n",
      "        [ 16.,  17.,  18.]])\n"
     ]
    }
   ],
   "source": [
    "r = torch.add(v, 10)\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  3.,   4.,   5.],\n",
       "        [  6.,   7.,   8.],\n",
       "        [  9.,  10.,  11.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = v+3\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L-P norm\n",
    "r = torch.dist(v, v+3, p=2)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  4.,  7.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.mean(v, 1)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  1],\n",
       "        [ 1,  1,  1],\n",
       "        [ 1,  1,  1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.eq(v, v)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.,  5.,  8.]), tensor([ 2,  2,  2]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.max(v, 1)\n",
    "r\n",
    "# first tuple stores the result, second tuple stores the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.]]), tensor([[ 0,  1,  2],\n",
       "         [ 0,  1,  2],\n",
       "         [ 0,  1,  2]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.sort(v, 1)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 3.,  4.,  5.]), tensor([ 1,  1,  1]))\n",
      "(tensor([ 1.,  4.,  7.]), tensor([ 1,  1,  1]))\n"
     ]
    }
   ],
   "source": [
    "# k-th element (start from 1) ascending order with corresponding index, along axis\n",
    "# (1 4 7\n",
    "# [torch.FloatTensor of size 3]\n",
    "# , 1 1 1\n",
    "# [torch.LongTensor of size 3]\n",
    "# )\n",
    "r = torch.kthvalue(v, 2, 0)\n",
    "print (r)\n",
    "r = torch.kthvalue(v, 2, 1)\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 2.],\n",
      "        [ 5.],\n",
      "        [ 8.]]), tensor([[ 2],\n",
      "        [ 2],\n",
      "        [ 2]]))\n",
      "(tensor([[ 6.,  7.,  8.],\n",
      "        [ 3.,  4.,  5.]]), tensor([[ 2,  2,  2],\n",
      "        [ 1,  1,  1]]))\n"
     ]
    }
   ],
   "source": [
    "# Top k (descending order) , (input, k, axis)\n",
    "# (\n",
    "#  2  5  8\n",
    "# [torch.FloatTensor of size 3x1]\n",
    "# ,\n",
    "#  2  2  2\n",
    "# [torch.LongTensor of size 3x1]\n",
    "# )\n",
    "r = torch.topk(v, 1, 1)\n",
    "print  (r)\n",
    "\n",
    "r = torch.topk(v,2,0)\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiplication\n",
    "mat = torch.randn(2, 4)\n",
    "vec = torch.randn(4)\n",
    "r = torch.mv(mat, vec)\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch multiplication\n",
    "batch1 = torch.randn(10, 3, 4)\n",
    "batch2 = torch.randn(10, 4, 5)\n",
    "r = torch.bmm(batch1, batch2)\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> x = torch.Tensor([[1], [2], [3]])\n",
    ">>> x.size()\n",
    "torch.Size([3, 1])\n",
    ">>> x.expand(3, 4)\n",
    " 1  1  1  1\n",
    " 2  2  2  2\n",
    " 3  3  3  3\n",
    "[torch.FloatTensor of size 3x4]\n",
    ">>> x.expand(-1, 4)   # -1 means not changing the size of that dimension\n",
    " 1  1  1  1\n",
    " 2  2  2  2\n",
    " 3  3  3  3\n",
    "[torch.FloatTensor of size 3x4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort a list and unsort it back (useful in pack and padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = [1,4,2,7,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  4.,  2.,  7.,  9.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  3,  1,  2,  0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1D case: use dim = 0 not dim =1\n",
    "_,idx_sort = torch.sort(torch.Tensor(A),dim = 0, descending=True)\n",
    "idx_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.,  7.,  4.,  2.,  1.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.Tensor(A)[idx_sort]\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  2,  3,  1,  0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,idx_unsort = torch.sort(idx_sort,dim = 0)\n",
    "idx_unsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  4.,  2.,  7.,  9.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = C[idx_unsort]\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad clipping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''''Parameters:parameters (Iterable[Tensor] or Tensor) – an iterable of Tensors or a single Tensor that will have gradients normalized\n",
    "clip_value (float or int) – maximum allowed value of the gradients The gradients are clipped in the range [-clip_value, clip_value]\"\n",
    "'''\n",
    "\n",
    "torch.nn.utils.clip_grad_value_(parameters, clip_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class torch.utils.data.Dataset\n",
    "\n",
    "    An abstract class representing a Dataset.\n",
    "\n",
    "    All other datasets should subclass it. All subclasses should override __len__, that provides the size of the dataset, and __getitem__, supporting integer indexing in range from 0 to len(self) exclusive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CoCoDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, transform, mode, batch_size, vocab_threshold, vocab_file, start_word, \n",
    "        end_word, unk_word, annotations_file, vocab_from_file, img_folder):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab = Vocabulary(vocab_threshold, vocab_file, start_word,\n",
    "            end_word, unk_word, annotations_file, vocab_from_file)\n",
    "        self.img_folder = img_folder\n",
    "        if self.mode == 'train':\n",
    "            self.coco = COCO(annotations_file)\n",
    "            self.ids = list(self.coco.anns.keys())\n",
    "            print('Obtaining caption lengths...')\n",
    "            all_tokens = [nltk.tokenize.word_tokenize(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in tqdm(np.arange(len(self.ids)))]\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "        else:\n",
    "            test_info = json.loads(open(annotations_file).read())\n",
    "            self.paths = [item['file_name'] for item in test_info['images']]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # obtain image and caption if in training mode\n",
    "        if self.mode == 'train':\n",
    "            ann_id = self.ids[index]\n",
    "            caption = self.coco.anns[ann_id]['caption']\n",
    "            img_id = self.coco.anns[ann_id]['image_id']\n",
    "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # Convert caption to tensor of word ids.\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            caption = []\n",
    "            caption.append(self.vocab(self.vocab.start_word))\n",
    "            caption.extend([self.vocab(token) for token in tokens])\n",
    "            caption.append(self.vocab(self.vocab.end_word))\n",
    "            caption = torch.Tensor(caption).long()\n",
    "\n",
    "            # return pre-processed image and caption tensors\n",
    "            return image, caption\n",
    "\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            path = self.paths[index]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            PIL_image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "            orig_image = np.array(PIL_image)\n",
    "            image = self.transform(PIL_image)\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return orig_image, image\n",
    "\n",
    "    def get_train_indices(self):\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return len(self.ids)\n",
    "        else:\n",
    "            return len(self.paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data loader source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#BatchSampler 是一个普通 Sampler 的 wrapper， 普通Sampler 一次仅产生一个 index， 而 BatchSampler 一次产生一个 batch 的 indices。\n",
    "class DataLoader(object):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, \n",
    "                 batch_sampler=None,\n",
    "                 num_workers=0, collate_fn=default_collate, pin_memory=False, \n",
    "                 drop_last=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.collate_fn = collate_fn\n",
    "        self.pin_memory = pin_memory\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        if batch_sampler is not None:\n",
    "            if batch_size > 1 or shuffle or sampler is not None or drop_last:\n",
    "                raise ValueError('batch_sampler is mutually exclusive with '\n",
    "                                 'batch_size, shuffle, sampler, and drop_last')\n",
    "\n",
    "        if sampler is not None and shuffle:\n",
    "            raise ValueError('sampler is mutually exclusive with shuffle')\n",
    "\n",
    "        if batch_sampler is None:\n",
    "            if sampler is None:\n",
    "                if shuffle:\n",
    "                    # dataset.__len__() 在 Sampler 中被使用。\n",
    "                    # 目的是生成一个 长度为 len(dataset) 的 序列索引（随机的）。\n",
    "                    sampler = RandomSampler(dataset)\n",
    "                else:\n",
    "                    # dataset.__len__() 在 Sampler 中被使用。\n",
    "                    # 目的是生成一个 长度为 len(dataset) 的 序列索引（顺序的）。\n",
    "                    sampler = SequentialSampler(dataset)\n",
    "            # Sampler 是个迭代器，一次之只返回一个 索引\n",
    "            # BatchSampler 也是个迭代器，但是一次返回 batch_size 个 索引\n",
    "            ## (batch_sampler (Sampler, optional) – like sampler, but returns a batch of indices at a time.\n",
    "            ## Mutually exclusive with batch_size, shuffle, sampler, and drop_last.)\n",
    "            batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
    "\n",
    "        self.sampler = sampler\n",
    "        self.batch_sampler = batch_sampler\n",
    "\n",
    "    def __iter__(self):\n",
    "        return DataLoaderIter(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample of data_loader\n",
    "def get_loader(transform,\n",
    "               mode='train',\n",
    "               batch_size=1,\n",
    "               vocab_threshold=None,\n",
    "               vocab_file='./vocab.pkl',\n",
    "               start_word=\"<start>\",\n",
    "               end_word=\"<end>\",\n",
    "               unk_word=\"<unk>\",\n",
    "               vocab_from_file=True,\n",
    "               num_workers=0,\n",
    "               cocoapi_loc='.'):\n",
    "    \"\"\"Returns the data loader.\n",
    "    Args:\n",
    "      transform: Image transform.\n",
    "      mode: One of 'train' or 'test'.\n",
    "      batch_size: Batch size (if in testing mode, must have batch_size=1).\n",
    "      vocab_threshold: Minimum word count threshold.\n",
    "      vocab_file: File containing the vocabulary. \n",
    "      start_word: Special word denoting sentence start.\n",
    "      end_word: Special word denoting sentence end.\n",
    "      unk_word: Special word denoting unknown words.\n",
    "      vocab_from_file: If False, create vocab from scratch & override any existing vocab_file.\n",
    "                       If True, load vocab from from existing vocab_file, if it exists.\n",
    "      num_workers: Number of subprocesses to use for data loading \n",
    "      cocoapi_loc: The location of the folder containing the COCO API: https://github.com/cocodataset/cocoapi\n",
    "    \"\"\"\n",
    "    \n",
    "    assert mode in ['train', 'test'], \"mode must be one of 'train' or 'test'.\"\n",
    "    if vocab_from_file==False: assert mode=='train', \"To generate vocab from captions file, must be in training mode (mode='train').\"\n",
    "\n",
    "    # Based on mode (train, val, test), obtain img_folder and annotations_file.\n",
    "    if mode == 'train':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(cocoapi_loc, 'cocoapi/images/train2014/')\n",
    "        annotations_file = os.path.join(cocoapi_loc, 'cocoapi/annotations/captions_train2014.json')\n",
    "    if mode == 'test':\n",
    "        assert batch_size==1, \"Please change batch_size to 1 if testing your model.\"\n",
    "        assert os.path.exists(vocab_file), \"Must first generate vocab.pkl from training data.\"\n",
    "        assert vocab_from_file==True, \"Change vocab_from_file to True.\"\n",
    "        img_folder = os.path.join(cocoapi_loc, 'cocoapi/images/test2014/')\n",
    "        annotations_file = os.path.join(cocoapi_loc, 'cocoapi/annotations/image_info_test2014.json')\n",
    "        \n",
    "     # COCO caption dataset.\n",
    "    dataset = CoCoDataset(transform=transform,\n",
    "                          mode=mode,\n",
    "                          batch_size=batch_size,\n",
    "                          vocab_threshold=vocab_threshold,\n",
    "                          vocab_file=vocab_file,\n",
    "                          start_word=start_word,\n",
    "                          end_word=end_word,\n",
    "                          unk_word=unk_word,\n",
    "                          annotations_file=annotations_file,\n",
    "                          vocab_from_file=vocab_from_file,\n",
    "                          img_folder=img_folder)\n",
    "\n",
    "    if mode == 'train':\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        # data loader for COCO dataset.\n",
    "        data_loader = data.DataLoader(dataset=dataset, \n",
    "                                      num_workers=num_workers,\n",
    "                                      batch_sampler=data.sampler.BatchSampler(sampler=initial_sampler,\n",
    "                                                                              batch_size=dataset.batch_size,\n",
    "                                                                              drop_last=False))\n",
    "    else:\n",
    "        data_loader = data.DataLoader(dataset=dataset,\n",
    "                                      batch_size=dataset.batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=num_workers)\n",
    "\n",
    "    return data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To load the data from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 以下两个代码是等价的\n",
    "for data in dataloader:\n",
    "    ...\n",
    "# 等价与\n",
    "iterr = iter(dataloader)\n",
    "while True:\n",
    "    try:\n",
    "        next(iterr)\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample 1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "            \n",
    "        \n",
    "        \n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, drop_prob = 0.4):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout = drop_prob, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        captions = captions[:,:-1]\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        #print (embeddings.shape)\n",
    "        #embeddings = embeddings.permute(1,0,2)\n",
    "        #print (embeddings.shape)\n",
    "        out,_ = self.lstm(embeddings)\n",
    "        #out = out.transpose(0,1)\n",
    "        #print (out.shape)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        \" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) \"\n",
    "        created_wordid = []\n",
    "        \n",
    "        A = random.randint(10,max_len)\n",
    "        for i in range(A):\n",
    "            out,_ = self.lstm(inputs)\n",
    "            output =self.linear(out.squeeze(1))\n",
    "            #output = self.linear(hiddens)\n",
    "            #output = output.squeeze(1)\n",
    "            wordid = output.max(1)[1]\n",
    "            prediction = wordid.item()\n",
    "            created_wordid.append(prediction)\n",
    "            inputs = self.embed(wordid)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        return created_wordid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n",
    "                      max_iter=100, power=0.9):\n",
    "    \"\"\"Polynomial decay of learning rate\n",
    "        :param init_lr is base learning rate\n",
    "        :param iter is a current iteration\n",
    "        :param lr_decay_iter how frequently decay occurs, default is 1\n",
    "        :param max_iter is number of maximum iterations\n",
    "        :param power is a polymomial power\n",
    "\n",
    "    \"\"\"\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return optimizer\n",
    "\n",
    "    lr = init_lr*(1 - iter/max_iter)**power\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
