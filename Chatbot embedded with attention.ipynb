{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assume our input is \" input1   target1\n",
    "#                       input2   target2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate pairs and normalize, filter the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "def normalize(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate\n",
    "lines = text.strip().split('\\n')\n",
    "pairs = [[normalize(s) for s in l.split('\\t')] for l in lines]\n",
    "# pairs = list(zip(input_sen,target_sen))\n",
    "# pairs = [list(i) for i in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter pairs by length of the \n",
    "def filter_pairs(pairs, MIN_LENGTH, MAX_LENGTH):\n",
    "    filtered_pairs = []\n",
    "    print (\"Initially read %d pairs\" % len(pairs))\n",
    "    for pair in pairs:\n",
    "        if len(pair[0]) >= MIN_LENGTH and len(pair[0]) <= MAX_LENGTH \\\n",
    "            and len(pair[1]) >= MIN_LENGTH and len(pair[1]) <= MAX_LENGTH:\n",
    "                filtered_pairs.append(pair)\n",
    "    print (\"filtered pairs %d\" %len(filtered_pairs))\n",
    "    return filtered_pairs\n",
    "\n",
    "pairs = filter_pairs(pairs, 1, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed: return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words %s / %s = %.4f' % (\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.index_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "SOS = 1\n",
    "EOS = 2\n",
    "\n",
    "class Lang:\n",
    "    def __intial__(self,name,trim_Bool):\n",
    "        self.name = name\n",
    "        self.trim = trim_bool\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:'PAD',1:'SOS',2:'EOS'}\n",
    "        self.n_words = 3\n",
    "        \n",
    "    def index_words(sentence):\n",
    "        return [index_word(s) for s in sentence.split(' ')]\n",
    "    \n",
    "    def index_word(word):\n",
    "        if word not in word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words+=1\n",
    "        else:\n",
    "            self.word2count[word] +=1\n",
    "    \n",
    "    def trim(self, min_count):\n",
    "        if not self.trim_bool: return\n",
    "        keep_word = []\n",
    "        for k,v in self.word2count.items():\n",
    "            if v>=min_count:\n",
    "                keep_word.append(k)\n",
    "                \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "vocab = Lang()\n",
    "for pair in pairs:\n",
    "    vocab.index_words(pair[0])\n",
    "    vocab.index_words(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out the non-frequent word\n",
    "MIN_COUNT = 3\n",
    "\n",
    "vocab.trim(MIN_COUNT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove those pairs with unknown word\n",
    "keep_pairs = []\n",
    "\n",
    "for pair in pairs:\n",
    "    input_sentence = pair[0]\n",
    "    output_sentence = pair[1]\n",
    "    keep_input = True\n",
    "    keep_output = True\n",
    "    \n",
    "    for word in input_sentence.split(' '):\n",
    "        if word not in input_lang.word2index:\n",
    "            keep_input = False\n",
    "            break\n",
    "\n",
    "    for word in output_sentence.split(' '):\n",
    "        if word not in output_lang.word2index:\n",
    "            keep_output = False\n",
    "            break\n",
    "\n",
    "    # Remove if pair doesn't match input and output conditions\n",
    "    if keep_input and keep_output:\n",
    "        keep_pairs.append(pair)\n",
    "\n",
    "print(\"Trimmed from %d pairs to %d, %.4f of total\" % (len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "pairs = keep_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [vocab.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate batches\n",
    "def random_batch(batch_size):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "\n",
    "    # Choose random pairs\n",
    "    for i in range(batch_size):\n",
    "        pair = random.choice(pairs, replace = False)\n",
    "        input_seqs.append(indexes_from_sentence(vocab, pair[0]))\n",
    "        target_seqs.append(indexes_from_sentence(vocab, pair[1]))\n",
    "    \n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    # Note that normally, we sort by input seqs, because there is a bi-directional LSTM/GRU there. For target, single \n",
    "    # direction does not always need pack_padeed\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_var = input_var.cuda()\n",
    "        target_var = target_var.cuda()\n",
    "        \n",
    "    return input_var, input_lengths, target_var, target_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        # Here only concatenate the two hidden states (bi-directions) of the layer 2. \n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        \n",
    "        # note hidden[-2,:,:] has a size of [batch size, hid. dim]        \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder for seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size  # input_size = vocaburary_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        '''' Note: we run this all at once (over multiple batches of multiple sequences)''''\n",
    "        # input_seqs : max_len,batch_size ; embedded: max_len,batch_size,embedding(hidden_size)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        # packed: max_len,batch_size,embedding(hidden_size)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # hidden: num_layer*num_direction,batch_size,hidden_size; \n",
    "        # outputs: max_len,batch_size,num_direct*hidden_size\n",
    "        # hidden: default to 0 if not provided \n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        #outputs: max_len,batch_size,hidden_size\n",
    "        #hidden: 2,batch_size,hidden_size\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder for CNN text - classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0],embedding_dim))\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1],embedding_dim))\n",
    "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2],embedding_dim))\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        x = x.permute(1, 0)\n",
    "                \n",
    "        #x = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "            \n",
    "        #conv_n = [batch size, n_channels, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conv_0, conv_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conv_1, conv_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conv_2, conv_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        #pooled_n = [batch size, n_channels]\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        #cat = [batch size, n_channels * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.conv1 = nn.Conv2d(1,32,5)\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(32,64,5)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.conv3 = nn.Conv2d(64,128,5)\n",
    "        self.dropout1= nn.Dropout2d(p = 0.6)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(,300)\n",
    "        self.dropout2 = nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(300,136)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.pool1(F.relu(self.conv1_bn(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.conv2_bn(self.conv2(x))))\n",
    "        x = x.view(x.size(0),-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder for CNN - image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # out = (input + 2*padding - kernel_size)/stride + 1\n",
    "\n",
    "        # 1 input image channel (grayscale), 32 output channels/feature maps, 5x5 square convolution kernel\n",
    "        # input 224, conv1_output (32,220,220), pool1_out (32,110,110)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        # input 110, conv2_out (64,107,107), pool2_out (64,53,53)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        # input 53, conv3_out (128,51,51), pool3_out (128,25,25)\n",
    "        self.conv3 = nn.Conv2d(64, 128,3)\n",
    "        self.dropout1 = nn.Dropout2d(p =0.6)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # flattened array 128*25*25 = 80000\n",
    "        #self.dropout1 = nn.Dropout(p =0.2)\n",
    "        self.fc1 = nn.Linear(128*25*25,300)\n",
    "        self.fc1_bn = nn.BatchNorm1d(300)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(300,136)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool1(F.relu(self.conv1_bn(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.conv2_bn(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.dropout1(self.conv3(x))))\n",
    "        x = x.view(x.size(0),-1)\n",
    "        #x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder for Amex solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class solution_encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, output_size, dropout, n_filters, filter_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, n_layers =1, bidirectional = True)\n",
    "        self.conv0 = nn.Conv2d(1, n_filters, (filter_size[0],hidden_size))\n",
    "        self.conv1 = nn.Conv2d(1, n_filters, (filter_size[1],hidden_size))\n",
    "        self.fc1 = nn.Linear(len(filter_size)*n_filters, output_size)\n",
    "        self.fc2 = nn.Linear((output_size+hidden_size),class_size)\n",
    "        \n",
    "    def forward(input_seq, input_lengths, hidden=None):\n",
    "        embedded  = self.embedding(input) # S*B*E\n",
    "        pack = utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(pack, hidden)\n",
    "        outputs, outputs_length = utils.rnn.pad_packed_sequence(outputs) #outputs  # S*B*2H, Hidden # 2*B*H\n",
    "        outputs = outputs[:,:,:self.hidden_size] + [:,:,self.hidden_size:]   # outputs  S*B*H\n",
    "        hidden = hidden[-2,:,:] + hidden[-1,:,:] # B*H\n",
    "        outputs = outputs.permute(1,0,2)  B*S*H\n",
    "        outputs = outputs.unsqueeze(1)   B*1*S*H\n",
    "        return outputs, hidden\n",
    "        \n",
    "class solution_decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, output_size, dropout, n_filters, filter_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(1, n_filters, (filter_size[0],hidden_size))\n",
    "        self.conv1 = nn.Conv2d(1, n_filters, (filter_size[1],hidden_size))\n",
    "        self.fc1 = nn.Linear(len(filter_size)*n_filters, output_size)\n",
    "    \n",
    "    def forward(encoder_output, hidden)\n",
    "        conv0 = F.relu(conv_0(outputs).squeeze(3))   #  B*n_filters*(S-filter_size)*1\n",
    "        conv1 = F.relu(conv_1(outputs).squeeze(3))   #  B*n_filters*(S-filter_size)*1\n",
    "        pool0 = F.max_pool1d(conv0, conv0.shape[2]).squeeze(2)\n",
    "        pool1 = F.max_pool1d(conv1, conv1.shape[2]).squeeze(2)  # B*n_filters\n",
    "        out = torch.cat((pool0,pool1),dim = 1)   # B*2n_filters\n",
    "        out = self.fc1(out)   # B*output_size\n",
    "        hidden = hidden # B*hidden_size\n",
    "        final_in = torch.cat((out,hidden), dim = 1) # B* (hidden_size+output_size)\n",
    "        final_out = self.fc2(final_in)\n",
    "        final_out = F.log_softmax(final_out)\n",
    "        \n",
    "        return final_out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for simple decoder we can run  all timesteps at once\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # output_size = vocaburary_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_seq, last_hidden):\n",
    "        '''' Note: we run this all steps at once (over multiple batches of multiple sequences)''''\n",
    "        # embedded: max_len,batch_size,embedding_size(hidden_size)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # outputs: max_len,batch_size, 1*hidden_size; hidden: 1*1, batch_size, hidden_size\n",
    "        outputs, hidden = self.gru(embedded,last_hidden)\n",
    "        # outputs: batch_size, max_len, 1*hidden_size\n",
    "        outputs = outputs.permute(1,0,2)\n",
    "        # outputs: batch_size, max_len, output_size(vocaburary_size)\n",
    "        outputs = self.out(outputs)\n",
    "        ouputs = F.log_softmax(outputs, dim =2)\n",
    "    \n",
    "    def sample(self,inputs,states = None, max_length = 20):\n",
    "        '''' Note: we run this one step at a time ''''\n",
    "        created_wordid = []\n",
    "        A = random.randint(10,max_length)\n",
    "        for i in range(A):\n",
    "            # inputs: seq_len(1), batch_len(1), embedding_size(hidden_size); states: 1,1,hidden_size\n",
    "            output, states = self.GRU(inputs,states)\n",
    "            #output: batch(1),seq(1),hidden_size\n",
    "            output = output.permute(1,0,2)\n",
    "            #output: batch(1),output_size(vocab_size)\n",
    "            output = self.out(output.squeeze(1))\n",
    "            _,wordid = output.max(dim = 1)\n",
    "            prediction = wordid.item()\n",
    "            created_wordid.append(prediction)\n",
    "            inputs = self.embed(wordid)\n",
    "            inputs = inputs.view(1,1,-1)\n",
    "        return created_wordid\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention decoder (Luang's attention)\n",
    "\n",
    "- Bahaum:   score is determined by last hidden state (t-1) and context\n",
    "- Luang:   score is determined by current RNN state (t) and all encoder outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        '''' Note: we run this one step at a time ''''\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        # input_seq: batch_size, seq_len(1)\n",
    "        batch_size = input_seq.size(0)\n",
    "        # embedded: batch_size, seq_len(1),hidden_size\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # embedded: seq_len(1), batch_size, hidden_size\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) \n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        # last_hidden/hidden: num_layer*direction(1),batch_size, hidden_size\n",
    "        # rnn_output: seq_len(1), batch_size, hidden_size\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        # rnn_output: seq_len(1), batch_size, hidden_size\n",
    "        # encoder_outputs: seq_len_encoder, batch_size, hidden_size\n",
    "        # attb_weights = batch_size, 1, seq_len_encoder\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # context: Batch_size x 1 x hidden_size\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) \n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> batch_size, hidden_size\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> batch_size, hidden_size\n",
    "        concat_input = torch.cat((rnn_output, context), 1) # concat_input: batch_size, 2*hidden_size\n",
    "        # feed input to a linear layer called concat\n",
    "        concat_output = F.tanh(self.concat(concat_input))   # concat_output: batch_size, hidden_size\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        # output: batch_size, output_size(vocaburary_size)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = rnn_output: seq_len(1), batch_size, hidden_size\n",
    "        # encoder_outputs: seq_len_encoder,batch_size,hidden_size\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        # attn_energies: batch_size, seq_len_encoder\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len))\n",
    "\n",
    "        if USE_CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                # hidden[:, b]: 1,hidden_size\n",
    "                # encoder_outputs[i, b].unsqueeze(0): 1, hidden_size\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to  Batch_size, 1, seq_len_encoder\n",
    "        return F.softmax(attn_energies).unsqueeze(1) \n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.v.dot(energy)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the net work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_batches, input_lengths, target_batches, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "    max_target_length = max(target_lengths)\n",
    "    # all_decoder_outputs: max_target_seq, batch_size, decoder\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size))\n",
    "\n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "    # Loss calculation and backpropagation\n",
    "    \"\"\"\"\"\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_lengths\n",
    "    )\n",
    "    \"\"\"\"\"\n",
    "    loss = cross_entropy(all_decoder_outputs.transpose(0, 1).contiguous(),target_batches.transpose(0, 1).contiguous()\n",
    "                        , ignore_index = 0)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0], ec, dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_model = 'dot'\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 100\n",
    "batch_size = 50\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 50000\n",
    "epoch = 0\n",
    "plot_every = 20\n",
    "print_every = 100\n",
    "evaluate_every = 1000\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers, dropout=dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout=dropout)\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(input_seq, max_length=MAX_LENGTH):\n",
    "    input_lengths = [len(input_seq)]\n",
    "    input_seqs = [indexes_from_sentence(input_lang, input_seq)]\n",
    "    input_batches = Variable(torch.LongTensor(input_seqs), volatile=True).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "        \n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
